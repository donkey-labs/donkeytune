# DonkeyTune
> Fine-tuning de petits LLMs sur Apple Silicon

```
       \    /
        \__/        
        (oo)\_______
        (__)\       )\/\
            ||-----|| 
v0.0.0      ||     ||
```


DonkeyTune est pipeline complet (un ensemble de scripts) pour fine-tuner des mod√®les sur MacBook Air/Pro M1/M2/M3/M4, avec export au format GGUF utilisables ensuite avec Docker Model Runner, llama.cpp, Ollama, ou tout outil compatible.

> **Notes**:
> - Sur un MacBook Air M4 32 GB, j'ai pu fine-tuner confortablement un mod√®le de 4 milliards de param√®tres.
> - üê£ ce sont mes baby steps en fine-tuning

üî• Si vous ne voulez pas tout lire, vous pouvez tenter le [QUICKSTART.md](./QUICKSTART.md)

## Comment √ßa marche

Le fine-tuning est r√©alis√© avec **LoRA** (Low-Rank Adaptation) via **mlx-lm**, le framework d'Apple optimis√© pour Apple Silicon. LoRA ne modifie pas directement les poids du mod√®le : il entra√Æne de petites matrices "adaptateurs" qui viennent se greffer sur le mod√®le existant. C'est ce qui rend le fine-tuning possible sur un laptop avec peu de m√©moire.

Le pipeline complet se d√©roule en 4 √©tapes :

```
Mod√®le HuggingFace + Dataset JSONL
        ‚îÇ
        ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  fine-tune  ‚îÇ  mlx_lm.lora : entra√Æne des adapters LoRA
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ    fuse     ‚îÇ  mlx_lm.fuse : fusionne adapters + mod√®le de base
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  convert    ‚îÇ  convert_hf_to_gguf.py : safetensors ‚Üí GGUF f16
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  quantize   ‚îÇ  llama-quantize : GGUF f16 ‚Üí GGUF Q4_K_M (plus petit)
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚ñº
    Fichier GGUF pr√™t pour Docker Model Runner, llama.cpp, Ollama, etc.
```

## Pr√©requis

- **macOS** sur Apple Silicon (M1, M2, M3, M4)
- **Python 3.10+** ‚Äî `setup.sh` d√©tecte automatiquement la bonne version ; si `python3` pointe vers Python 3.9 (d√©faut Xcode CLT), installez une version plus r√©cente : `brew install python@3.12`
- **Homebrew** (pour installer cmake)
- **cmake** : `brew install cmake`
- **git**
- Environ **5 Go d'espace disque** (pour le venv, llama.cpp et les mod√®les)

V√©rifiez votre architecture :

```bash
uname -m
# Doit afficher : arm64
```

## Installation

```bash
git clone https://github.com/donkey-labs/donkeytune.git
cd donkeytune
make setup
```

Le script `setup.sh` installe automatiquement :

| Composant | R√¥le |
|-----------|------|
| `.venv/` | Virtual environment Python isol√© |
| `mlx-lm` | Framework Apple MLX pour fine-tuning et inf√©rence |
| `torch` | PyTorch (n√©cessaire pour la conversion GGUF) |
| `gguf`, `numpy`, `sentencepiece` | D√©pendances de conversion |
| `llama.cpp/` | Clon√© et compil√© localement (pour `llama-quantize` et `convert_hf_to_gguf.py`) |
| `data/`, `output/` | R√©pertoires de travail |

> Selon les machines, l'installation prend environ 5 minutes (t√©l√©chargement + compilation).

### Token HuggingFace (optionnel)

Certains mod√®les sur HuggingFace sont prot√©g√©s (gated models) et n√©cessitent un token d'acc√®s. D'autres sont publics mais le token permet d'√©viter les limites de t√©l√©chargement.

Cr√©ez un fichier `.env` √† la racine du projet :

```bash
HF_TOKEN=hf_votre_token_ici
```

Vous pouvez obtenir votre token sur [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).

Les scripts chargent automatiquement ce fichier √† chaque ex√©cution. Le `.env` est exclu du d√©p√¥t git via `.gitignore`.

## Pr√©parer le dataset

Pour fine-tuner un mod√®le, vous devez lui fournir des exemples d'entra√Ænement : des paires question/r√©ponse qui montrent au mod√®le ce que vous attendez de lui. Ces exemples sont organis√©s dans un dataset au format JSONL.

### Format des fichiers

Le dataset doit √™tre au format **JSONL** (JSON Lines : un objet JSON par ligne). Chaque ligne repr√©sente un **exemple d'entra√Ænement** au format chat, avec exactement 3 messages :

```json
{
  "messages": [
    {
      "role": "system",
      "content": "You are a Go programming assistant."
    },
    {
      "role": "user",
      "content": "Write a function that reverses a string"
    },
    {
      "role": "assistant",
      "content": "func ReverseString(s string) string {\n\trunes := []rune(s)\n\tfor i, j := 0, len(runes)-1; i < j; i, j = i+1, j-1 {\n\t\trunes[i], runes[j] = runes[j], runes[i]\n\t}\n\treturn string(runes)\n}"
    }
  ]
}
```

> **Rappel** : dans le fichier `.jsonl`, chaque exemple reste sur **une seule ligne**. Le format ci-dessus est affich√© en pretty-print pour faciliter la lecture.

```mermaid
sequenceDiagram
    participant S as üõ†Ô∏è System
    participant U as üë§ User
    participant A as ü§ñ Assistant

    Note over S,A: One training example (one JSONL line)

    S->>A: Sets the role<br/>"You are a Go programming assistant."
    U->>A: Asks the question<br/>"Write a function that reverses a string"
    A-->>U: Provides the expected answer<br/>func ReverseString(s string) string { ... }

    Note over S,A: The model learns to reproduce<br/>the assistant's response
```

| R√¥le | Contenu | Conseils |
|------|---------|----------|
| `system` | Instruction de comportement | **Identique** dans tous les exemples. D√©finit la personnalit√© du mod√®le. |
| `user` | La question / demande en langage naturel | Variez les formulations pour que le mod√®le g√©n√©ralise. |
| `assistant` | La r√©ponse attendue (le code, l'explication, etc.) | C'est ce que le mod√®le apprend √† reproduire. Soignez la qualit√©. |

> **Important** : chaque ligne est un JSON complet sur une seule ligne. Pas de retour √† la ligne entre les accolades. Les retours √† la ligne *dans* le code doivent √™tre encod√©s en `\n` et les tabulations en `\t`.

### Les deux fichiers : train.jsonl et valid.jsonl

```
data/
‚îú‚îÄ‚îÄ train.jsonl    # Training samples
‚îî‚îÄ‚îÄ valid.jsonl    # Validation samples
```

**`train.jsonl`** ‚Äî Le fichier d'entra√Ænement. C'est le c≈ìur du dataset : le mod√®le apprend √† partir de ces exemples. √Ä chaque it√©ration du fine-tuning, un batch d'exemples est tir√© de ce fichier, et le mod√®le ajuste ses poids pour mieux reproduire les r√©ponses attendues.

**`valid.jsonl`** ‚Äî Le fichier de validation. Ces exemples ne sont **jamais utilis√©s pour l'entra√Ænement**. Le mod√®le est √©valu√© dessus p√©riodiquement (toutes les 50 it√©rations par d√©faut) pour calculer la **validation loss**. C'est un indicateur indispensable :

- Si la validation loss **diminue** en m√™me temps que la training loss ‚Üí le mod√®le **apprend** et g√©n√©ralise bien.
- Si la validation loss **monte** alors que la training loss descend ‚Üí c'est de l'**overfitting** : le mod√®le m√©morise les exemples d'entra√Ænement au lieu d'apprendre les patterns. Il faut alors r√©duire le nombre d'it√©rations ou augmenter le dataset.

Sans fichier de validation, vous n'avez aucun moyen de savoir si le mod√®le g√©n√©ralise ou s'il m√©morise.

### Comment construire son dataset

**1. D√©finir le system prompt**

Choisissez une instruction claire et gardez-la identique dans tous les exemples :

```
You are a Go programming assistant. Generate clean, idiomatic Go code.
```

**2. √âcrire les paires question/r√©ponse**

Chaque exemple est une paire : une question (`user`) et la r√©ponse id√©ale (`assistant`). Quelques principes :

- **Diversit√©** : couvrez diff√©rents aspects de votre domaine (fonctions simples, structs, interfaces, concurrence, HTTP, fichiers, patterns de conception...).
- **Qualit√©** : la r√©ponse `assistant` est ce que le mod√®le va reproduire. Assurez-vous que le code compile, est idiomatique, et suit les conventions du langage.
- **Variation des formulations** : formulez les questions de plusieurs fa√ßons. "Write a function that...", "Implement a...", "Create a...".
- **Difficult√© progressive** : m√©langez des exemples simples (ReverseString) et complexes (Circuit Breaker pattern, LRU cache).

**3. S√©parer train et validation**

Le split recommand√© est **80% train / 20% valid**. Les exemples de validation doivent √™tre **diff√©rents** de ceux d'entra√Ænement, mais couvrir le m√™me type de t√¢ches. Ne mettez pas les exemples les plus faciles d'un c√¥t√© et les plus durs de l'autre : m√©langez la difficult√©.

**4. Formater en JSONL**

Chaque ligne = un JSON complet. Les sauts de ligne dans le code sont encod√©s en `\n`, les tabulations en `\t` :

```json
{"messages":[{"role":"system","content":"You are a Go programming assistant. Generate clean, idiomatic Go code."},{"role":"user","content":"Write a function that checks if a number is prime"},{"role":"assistant","content":"func IsPrime(n int) bool {\n\tif n < 2 {\n\t\treturn false\n\t}\n\tfor i := 2; i*i <= n; i++ {\n\t\tif n%i == 0 {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}"}]}
```

### Tailles recommand√©es

| Exemples | Usage |
|----------|-------|
| **50-100** | Test rapide, v√©rifier que le pipeline fonctionne |
| **200-500** | Fine-tuning l√©ger, sp√©cialisation sur un domaine pr√©cis |
| **500-2000** | Fine-tuning significatif, bonne g√©n√©ralisation |
| **2000+** | Fine-tuning approfondi, r√©sultats de haute qualit√© |

Plus le dataset est grand et diversifi√©, meilleur sera le r√©sultat. Mais m√™me 100 exemples de bonne qualit√© donnent des r√©sultats visibles.

## Choisir un mod√®le

Le choix du mod√®le de base est la d√©cision la plus importante. C'est lui qui d√©termine la qualit√© de d√©part, la m√©moire n√©cessaire, et le temps de fine-tuning.

### Crit√®res de choix

**1. Le format : safetensors obligatoire**

Le mod√®le doit √™tre au format **safetensors** (le format standard HuggingFace). Les mod√®les au format GGUF ne fonctionnent **pas** pour le fine-tuning. Sur HuggingFace, v√©rifiez la pr√©sence de fichiers `*.safetensors` dans l'onglet "Files". Si vous ne voyez que des fichiers `.gguf`, cherchez la version safetensors du m√™me mod√®le (souvent sans le suffixe `-gguf`).

**2. La taille : adapter √† votre RAM**

La taille du mod√®le d√©termine la m√©moire n√©cessaire pendant le fine-tuning. Le mod√®le est charg√© en m√©moire unifi√©e (RAM partag√©e entre CPU et GPU sur Apple Silicon), et LoRA ajoute un surco√ªt proportionnel aux param√®tres `RANK` et `NUM_LAYERS`.

| RAM disponible | Taille max recommand√©e | Exemples |
|---------------|----------------------|----------|
| 8 GB | 0.5B | Gemma 3 270M-IT, Qwen2.5-Coder-0.5B-Instruct |
| 16 GB | 1-2B | Qwen2.5-1.5B-Instruct, Qwen2.5-Coder-1.5B-Instruct, Lucy-128k, Llama-3.2-1B |
| 24 GB | 3-4B | SmolLM3-3B, Jan-nano (serr√©) |
| 32 GB+ | 4-8B | Mod√®les 7B-8B avec param√®tres r√©duits (rnj-1-instruct 8.3B) |

> **R√®gle empirique** : comptez environ **3x la taille du mod√®le en GB** pour le fine-tuning LoRA (mod√®le + adapters + buffers). Un mod√®le 0.5B (~1 GB) utilise ~3.7 GB ; un mod√®le 3B (~6 GB) utilise ~12-14 GB.

**3. L'architecture : doit √™tre support√©e par mlx-lm**

mlx-lm supporte les architectures les plus courantes : Qwen2, Qwen3, Llama, Mistral, Phi, SmolLM, Gemma, etc. Si un mod√®le utilise une architecture exotique ou tr√®s r√©cente, il peut ne pas √™tre support√©.

**4. Instruct vs Base**

- **Instruct** (recommand√©) : le mod√®le a d√©j√† √©t√© entra√Æn√© √† suivre des instructions (format question/r√©ponse). Le fine-tuning affine ce comportement existant. R√©sultats plus rapides et plus pr√©visibles.
- **Base** : le mod√®le a uniquement √©t√© pr√©-entra√Æn√© sur du texte brut. Il faut lui apprendre le format instruction/r√©ponse en plus de la sp√©cialisation. N√©cessite plus de donn√©es et d'it√©rations.

> Privil√©giez les mod√®les **Instruct** sauf si vous avez un cas d'usage tr√®s sp√©cifique.

**5. La sp√©cialisation du mod√®le de base**

Si votre fine-tuning porte sur du code, choisissez un mod√®le d√©j√† sp√©cialis√© en code (ex: `Qwen2.5-Coder`) plut√¥t qu'un mod√®le g√©n√©raliste. Le fine-tuning est un **affinage** : il fonctionne mieux quand le mod√®le de base a d√©j√† des comp√©tences proches de ce que vous voulez lui enseigner.

### Comment trouver un mod√®le sur HuggingFace

1. Allez sur [huggingface.co/models](https://huggingface.co/models)
2. Filtrez par :
   - **Tasks** : Text Generation
   - **Libraries** : Safetensors
   - **Sort** : Trending ou Most Downloads
3. V√©rifiez la taille du mod√®le (dans le nom ou la fiche)
4. V√©rifiez que les fichiers `*.safetensors` sont pr√©sents dans l'onglet "Files"

### Petit vs Grand : le compromis

Un mod√®le plus petit (0.5B-1B) :
- Fine-tuning rapide (minutes au lieu d'heures)
- Peu de m√©moire n√©cessaire
- It√©rations rapides pour exp√©rimenter
- R√©sultats corrects mais limit√©s sur les t√¢ches complexes

Un mod√®le plus grand (3B-4B) :
- Fine-tuning plus long
- Plus de m√©moire n√©cessaire
- Meilleure compr√©hension du contexte et raisonnement
- R√©sultats de meilleure qualit√©, surtout sur les t√¢ches complexes

> **Conseil** : commencez avec un mod√®le 0.5B pour valider votre pipeline et votre dataset. Une fois satisfait du processus, passez √† un mod√®le plus grand pour de meilleurs r√©sultats.

## Comprendre et ajuster les param√®tres de fine-tuning

Les param√®tres contr√¥lent **comment** le mod√®le apprend. De mauvais param√®tres peuvent gaspiller du temps (trop d'it√©rations), saturer la m√©moire (batch trop grand), ou produire un mod√®le inutile (learning rate trop √©lev√©).

### Vue d'ensemble

| Param√®tre | D√©faut | Quoi | Impact principal |
|-----------|--------|------|-----------------|
| `ITERS` | `600` | Nombre total de passes d'entra√Ænement | Dur√©e, risque d'overfitting |
| `BATCH_SIZE` | `4` | Exemples trait√©s en parall√®le | M√©moire, stabilit√© |
| `NUM_LAYERS` | `8` | Couches du mod√®le modifi√©es par LoRA | M√©moire, capacit√© d'adaptation |
| `LR` | `1e-5` | Vitesse d'apprentissage | Convergence, stabilit√© |
| `RANK` | `8` | Taille des matrices LoRA | M√©moire, capacit√© d'adaptation |

### ITERS ‚Äî Nombre d'it√©rations

C'est le nombre de fois que le mod√®le voit un batch d'exemples et ajuste ses poids. Chaque it√©ration traite `BATCH_SIZE` exemples.

**Comment choisir :**

Le nombre d'it√©rations d√©pend de la taille de votre dataset. Le mod√®le doit voir chaque exemple **plusieurs fois** (on appelle √ßa des "epochs"). Le nombre d'epochs est :

```
epochs = (ITERS √ó BATCH_SIZE) / nombre_exemples_train
```

| Dataset | ITERS recommand√© | Epochs (~) |
|---------|-----------------|-----------|
| 50 exemples | 300-500 | 24-40 |
| 100 exemples | 500-800 | 20-32 |
| 500 exemples | 1000-2000 | 8-16 |
| 2000 exemples | 2000-4000 | 4-8 |

**Signes que vous avez trop d'it√©rations (overfitting) :**
- La train loss descend √† ~0.001 ou moins
- La validation loss **remonte** apr√®s avoir descendu
- Le mod√®le r√©p√®te mot pour mot les exemples du dataset au lieu de g√©n√©raliser

**Signes que vous n'en avez pas assez :**
- La train loss est encore √©lev√©e (> 0.5)
- Les r√©ponses du mod√®le sont incoh√©rentes ou ne suivent pas le pattern du dataset

> **Strat√©gie** : commencez avec la valeur par d√©faut (600). Observez la validation loss. Si elle monte vers la fin, r√©duisez. Si la train loss est encore haute, augmentez.

### BATCH_SIZE ‚Äî Taille du batch

Le nombre d'exemples trait√©s en parall√®le √† chaque it√©ration. Tous ces exemples sont charg√©s en m√©moire simultan√©ment.

**Comment choisir :**

- **Mod√®le 0.5B-1B** : `BATCH_SIZE=4` (par d√©faut) fonctionne bien
- **Mod√®le 3B** : r√©duisez √† `BATCH_SIZE=2`
- **Mod√®le 4B+** : utilisez `BATCH_SIZE=1`
- Si vous avez un `Out of Memory` : r√©duisez le batch size en premier

> Un batch plus grand donne un gradient plus stable (le mod√®le apprend plus r√©guli√®rement), mais co√ªte plus de m√©moire. Un batch de 1 est "bruyant" (le mod√®le oscille plus) mais consomme le minimum de m√©moire.

### NUM_LAYERS ‚Äî Nombre de couches LoRA

LoRA ne modifie pas tout le mod√®le : il ajoute des adaptateurs sur un certain nombre de couches (layers) du transformer. Ce param√®tre contr√¥le combien de couches sont modifi√©es.

**Nombre total de couches par mod√®le :**

| Mod√®le | Couches totales | NUM_LAYERS recommand√© |
|--------|----------------|---------------------|
| Qwen2.5-Coder-0.5B | 24 | 8 (d√©faut) |
| Qwen2.5-1.5B / Qwen2.5-Coder-1.5B | 28 | 8 |
| SmolLM3-3B | 36 | 4-8 |


**Comment choisir :**

- **Plus de couches** = plus de capacit√© d'adaptation, mais plus de m√©moire et plus lent
- **Moins de couches** = moins de m√©moire, plus rapide, mais le mod√®le a moins de "place" pour apprendre
- `-1` = appliquer LoRA √† **toutes** les couches (maximum de capacit√©, maximum de m√©moire)

> **R√®gle** : pour un mod√®le petit (0.5B-1B), 8 couches suffisent. Pour un mod√®le plus gros (3B+), r√©duisez √† 4 pour rester dans les limites de m√©moire. Augmentez si les r√©sultats sont insuffisants et que vous avez de la marge en RAM.

### LR (Learning Rate) ‚Äî Vitesse d'apprentissage

Le learning rate contr√¥le **l'amplitude des ajustements** √† chaque it√©ration. C'est le param√®tre le plus d√©licat : trop √©lev√©, le mod√®le "oublie" ce qu'il savait ; trop bas, il n'apprend rien.

**√âchelle des valeurs :**

| Learning Rate | Comportement | Usage |
|--------------|-------------|-------|
| `5e-6` | Tr√®s prudent, lent | Gros mod√®les (4B+), petits datasets |
| `1e-5` | **Recommand√©** (d√©faut) | La plupart des cas |
| `2e-5` | Apprentissage plus rapide | Datasets volumineux (500+), mod√®les petits |
| `5e-5` | Agressif | Risque d'instabilit√©, uniquement si 1e-5 est trop lent |
| `1e-4` | Dangereux | D√©conseill√©, le mod√®le va "oublier" ses connaissances de base |

**Comment choisir :**

- `1e-5` est un bon d√©faut pour la quasi-totalit√© des cas
- Si la train loss descend tr√®s lentement apr√®s 200 it√©rations ‚Üí augmentez l√©g√®rement (`2e-5`)
- Si la train loss oscille fortement au lieu de descendre ‚Üí r√©duisez (`5e-6`)
- Les mod√®les plus gros sont plus sensibles au learning rate ‚Üí restez conservateur

> **Ne pas toucher sauf n√©cessit√©.** Le d√©faut `1e-5` fonctionne bien dans 90% des cas. Ajustez les autres param√®tres en premier (ITERS, BATCH_SIZE, RANK) avant de modifier le learning rate.

### RANK ‚Äî Rang LoRA

Le rang LoRA d√©termine la **taille des matrices adaptateurs**. Un rang plus √©lev√© donne √† LoRA plus de "capacit√©" pour modifier le comportement du mod√®le.

**Impact concret :**

| Rang | Param√®tres ajout√©s | M√©moire ajout√©e | Capacit√© |
|------|-------------------|-----------------|----------|
| 4 | ~0.3% du mod√®le | Faible | Ajustements l√©gers |
| 8 | ~0.6% du mod√®le | Mod√©r√©e | **Bon compromis** (d√©faut) |
| 16 | ~1.2% du mod√®le | Significative | Forte adaptation |
| 32 | ~2.4% du mod√®le | √âlev√©e | Modification profonde |
| 64 | ~4.8% du mod√®le | Tr√®s √©lev√©e | Quasi full fine-tuning |

**Comment choisir :**

- **Rank 4** : suffisant pour des t√¢ches simples (changer le style de r√©ponse, ajouter un system prompt sp√©cifique)
- **Rank 8** (d√©faut) : bon pour la plupart des cas (sp√©cialisation sur un langage, Q&A sur un domaine)
- **Rank 16** : pour des t√¢ches plus exigeantes (apprendre un nouveau langage de programmation, forte sp√©cialisation)
- **Rank 32+** : rarement n√©cessaire, sauf si rank 16 ne suffit pas et que vous avez assez de m√©moire

> **Note** : augmenter le rang augmente aussi le risque d'overfitting si le dataset est petit. Un rank 16 avec 50 exemples risque de m√©moriser au lieu d'apprendre. Augmentez le rang ET le dataset ensemble.

### Param√®tres avanc√©s (dans le config YAML)

Le script g√©n√®re un fichier de configuration YAML avec des param√®tres suppl√©mentaires :

```yaml
lora_parameters:
  rank: 8         # Le RANK choisi
  alpha: 16       # Automatiquement calcul√© : RANK √ó 2
  dropout: 0.05   # R√©gularisation pour limiter l'overfitting
  scale: 10.0     # Facteur d'√©chelle des adaptateurs
```

- **alpha** : facteur de mise √† l'√©chelle. La r√®gle `alpha = rank √ó 2` est standard et ne n√©cessite pas d'ajustement.
- **dropout** : √† chaque it√©ration, 5% des poids LoRA sont al√©atoirement d√©sactiv√©s. Cela force le mod√®le √† ne pas trop d√©pendre d'un seul poids, ce qui r√©duit l'overfitting. Augmentez √† `0.1` si vous constatez de l'overfitting avec un petit dataset.
- **scale** : multiplie l'effet des adaptateurs. `10.0` est un bon d√©faut.

### Recettes par taille de mod√®le

Voici les combinaisons de param√®tres recommand√©es en fonction de la taille du mod√®le et de la RAM :

**Mod√®le 0.3B-1B (8-16 GB RAM)**

```bash
make train MODEL=unsloth/gemma-3-270m-it
```

| Param√®tre | Valeur |
|-----------|--------|
| BATCH_SIZE | 4 |
| NUM_LAYERS | 8 |
| RANK | 8 |
| LR | 1e-5 |
| ITERS | 600 |

**Mod√®le 1.5B (16 GB RAM)**

```bash
make train MODEL=unsloth/Qwen2.5-1.5B-Instruct
# ou pour du code :
make train MODEL=unsloth/Qwen2.5-Coder-1.5B-Instruct
```

| Param√®tre | Valeur | Pourquoi |
|-----------|--------|----------|
| BATCH_SIZE | 4 | Assez de m√©moire |
| NUM_LAYERS | 8 | Les mod√®les Qwen2.5-1.5B ont 28 couches, 8 suffit |
| RANK | 8 | Bon compromis |
| LR | 1e-5 | Standard |
| ITERS | 600 | √Ä ajuster selon le dataset |

**Mod√®le 3B (24 GB RAM)**

```bash
make train MODEL=unsloth/SmolLM3-3B-128K BATCH_SIZE=2 NUM_LAYERS=4
```

| Param√®tre | Valeur | Pourquoi |
|-----------|--------|----------|
| BATCH_SIZE | **2** | R√©duire pour √©conomiser la m√©moire |
| NUM_LAYERS | **4** | Moins de couches = moins de m√©moire |
| RANK | 8 | On peut garder le d√©faut |
| LR | 1e-5 | Standard |
| ITERS | 600 | Peut augmenter si la loss est encore haute |

**Mod√®le 4B (24 GB RAM, serr√©)**

| Param√®tre | Valeur | Pourquoi |
|-----------|--------|----------|
| BATCH_SIZE | **1** | Minimum pour tenir en m√©moire |
| NUM_LAYERS | **4** | Minimum raisonnable |
| RANK | 8 | R√©duire √† 4 si Out of Memory |
| LR | **5e-6** | Plus prudent avec un gros mod√®le |
| ITERS | 600 | Sera plus lent, ajuster √† la loss |

**Mod√®le 8B (32 GB+ RAM, exp√©rimental sur 24 GB)**

| Param√®tre | Valeur | Pourquoi |
|-----------|--------|----------|
| BATCH_SIZE | **1** | Indispensable, le mod√®le occupe d√©j√† ~16 GB |
| NUM_LAYERS | **2** | Minimum absolu pour limiter la m√©moire LoRA |
| RANK | **4** | R√©duire pour √©conomiser la m√©moire |
| LR | **5e-6** | Tr√®s prudent, un 8B est sensible au learning rate |
| ITERS | 600 | Peut √™tre lent (~2-3h), ajuster selon la loss |


### Diagnostic : que faire si...

| Probl√®me | Cause probable | Solution |
|----------|---------------|----------|
| `Out of Memory` | Mod√®le trop gros pour la RAM | R√©duire BATCH_SIZE, NUM_LAYERS, ou RANK |
| Train loss ne descend pas | Learning rate trop bas, ou trop peu d'it√©rations | Augmenter LR √† `2e-5`, ou augmenter ITERS |
| Train loss oscille fortement | Learning rate trop √©lev√©, ou batch trop petit | R√©duire LR √† `5e-6`, ou augmenter BATCH_SIZE |
| Val loss remonte (overfitting) | Trop d'it√©rations, ou dataset trop petit | R√©duire ITERS, augmenter le dataset, ou augmenter dropout |
| Train loss ~0 mais le mod√®le est mauvais | Overfitting s√©v√®re | Le mod√®le a m√©moris√©. R√©duire ITERS et RANK, augmenter le dataset |
| R√©sultats m√©diocres malgr√© bonne loss | Rank trop bas, ou mod√®le de base inadapt√© | Augmenter RANK √† 16, ou choisir un meilleur mod√®le de base |
| Fine-tuning tr√®s lent | Mod√®le gros + beaucoup de couches | R√©duire NUM_LAYERS, fermer les autres applications |

## Lancer le fine-tuning

```bash
make train MODEL=<model_name> 
```

Par exemple :

```bash
make train MODEL=unsloth/Qwen2.5-1.5B-Instruct

# Ajuster les hyperparam√®tres
make train MODEL=unsloth/Qwen2.5-1.5B-Instruct ITERS=300 BATCH_SIZE=2 LR=2e-5 RANK=16
```

Convertir ensuite en GGUF :

```bash
make convert MODEL=unsloth/Qwen2.5-1.5B-Instruct
```

### Ce qui se passe pendant le fine-tuning

Le script affiche la progression toutes les 10 it√©rations :

```
Iter 50: Train loss 0.440, Learning Rate 1.000e-05, It/sec 1.695, Tokens/sec 1086
```

- **Train loss** : doit diminuer. Passe typiquement de ~2.0 √† ~0.05.
- **Val loss** : la loss sur les donn√©es de validation. Si elle monte alors que train loss descend, c'est de l'**overfitting** (le mod√®le m√©morise au lieu d'apprendre).
- **It/sec** : vitesse d'entra√Ænement.
- **Peak mem** : m√©moire utilis√©e. Un mod√®le 0.5B utilise environ 3.7 GB.

Les adapters sont sauvegard√©s toutes les 100 it√©rations dans `output/adapters/`.

## Avant de relancer un fine-tuning

Les adapters produits par un fine-tuning sont **li√©s au mod√®le qui les a g√©n√©r√©s**. Si vous changez de mod√®le, de dataset ou de param√®tres LoRA (rank, num-layers), vous **devez** nettoyer les fichiers de sortie avant de relancer. Sinon, vous obtiendrez des erreurs du type `IndexError: list index out of range` au moment du test ou de la conversion.

### Changer de mod√®le

```bash
# Nettoyer toutes les sorties (adapters, mod√®le fusionn√©, GGUF)
make clean-output

# Relancer avec le nouveau mod√®le
make train MODEL=unsloth/gemma-3-270m-it
make convert MODEL=unsloth/gemma-3-270m-it
```

### Changer de dataset

Si vous gardez le m√™me mod√®le mais changez le dataset, m√™me principe :

```bash
# Nettoyer les sorties
make clean-output

# Remplacer les fichiers dans data/
# ... puis relancer
make train MODEL=unsloth/gemma-3-270m-it
```

### Changer les hyperparam√®tres (rank, num-layers)

Si vous changez `RANK` ou `NUM_LAYERS`, les adapters ne seront plus compatibles avec les pr√©c√©dents :

```bash
make clean-output
make train MODEL=unsloth/gemma-3-270m-it RANK=16 NUM_LAYERS=4
```

### Relancer avec le m√™me mod√®le et les m√™mes param√®tres

Dans ce cas, le fine-tuning **√©crase** les adapters existants. Pas besoin de nettoyer au pr√©alable.

### R√®gle simple

En cas de doute, lancez `make clean-output` avant de relancer. Cela ne supprime que les fichiers g√©n√©r√©s (adapters, mod√®le fusionn√©, GGUF). Vos scripts et votre dataset ne sont pas touch√©s.

## Enrichir un mod√®le d√©j√† fine-tun√© avec un nouveau dataset
> ‚úã **Exp√©rimental**, en cours de d√©veloppement üöß

Vous avez fine-tun√© un mod√®le et obtenu vos fichiers GGUF. Maintenant vous voulez lui enseigner de nouvelles connaissances avec un dataset suppl√©mentaire. Trois m√©thodes sont disponibles, chacune adapt√©e √† un contexte diff√©rent.

### Fusionner les datasets


Le script `merge-datasets.sh` combine les fichiers `train.jsonl` et `valid.jsonl` de plusieurs dossiers de datasets en un seul :

```bash
make merge-data INPUTS="./datasets/goloscript ./datasets/hawaiian-pizza"
```

Par d√©faut, les fichiers fusionn√©s sont √©crits dans `./data`. Vous pouvez changer la destination avec `DATA_DIR` :

```bash
make merge-data INPUTS="./datasets/goloscript ./datasets/hawaiian-pizza" DATA_DIR=./output/merged-data
```

### M√©thode A : Fusionner et repartir de z√©ro (le plus fiable)

On combine tous les datasets, puis on relance un fine-tuning complet depuis le mod√®le de base. C'est la m√©thode la plus fiable : aucun risque d'oubli (catastrophic forgetting), le mod√®le voit toutes les donn√©es.

```bash
make merge-data INPUTS="./datasets/goloscript ./datasets/new-topic"
make clean-output
make train MODEL=unsloth/gemma-3-270m-it
make convert MODEL=unsloth/gemma-3-270m-it
```

**Quand l'utiliser** : quand le nouveau dataset change significativement le domaine ou le ton, ou quand vous voulez le r√©sultat le plus pr√©visible. C'est aussi la bonne approche pour un premier enrichissement.

**Inconv√©nient** : c'est le plus long, car tout l'entra√Ænement est refait depuis le d√©but.

### M√©thode B : Resume seul (le plus rapide)

On reprend les adapters LoRA existants et on continue l'entra√Ænement avec uniquement le nouveau dataset. C'est la m√©thode la plus rapide.

```bash
make train-resume MODEL=unsloth/gemma-3-270m-it \
  DATA_DIR=./datasets/new-topic \
  RESUME_FROM=./output/adapters/adapters.safetensors
make convert MODEL=unsloth/gemma-3-270m-it
```

Le param√®tre `RESUME_FROM` pointe vers le fichier `adapters.safetensors` produit par le fine-tuning pr√©c√©dent. L'entra√Ænement reprend l√† o√π il s'√©tait arr√™t√©, avec les nouvelles donn√©es.

**Quand l'utiliser** : quand vous ajoutez des donn√©es compl√©mentaires dans le m√™me domaine (ex: nouvelles Q&A GoloScript) et que les poids existants sont un bon point de d√©part.

**Inconv√©nient** : risque de "catastrophic forgetting" ‚Äî le mod√®le peut partiellement oublier l'ancien sujet en apprenant le nouveau.

### M√©thode C : Fusionner + resume (meilleur compromis)

On combine ancien et nouveau dataset, puis on reprend les adapters existants. Le mod√®le revoit tout l'ancien contenu mais d√©marre avec un avantage (les anciens poids LoRA).

```bash
make train-merged MODEL=unsloth/gemma-3-270m-it \
  INPUTS="./datasets/goloscript ./datasets/new-topic" \
  RESUME_FROM=./output/adapters/adapters.safetensors
make convert
```

**Quand l'utiliser** : quand vous voulez le meilleur des deux mondes ‚Äî pas de risque d'oubli gr√¢ce √† la fusion des donn√©es, et un entra√Ænement plus rapide gr√¢ce au resume.

### R√©sum√© des m√©thodes

| M√©thode | Commande | Vitesse | Risque d'oubli | Fiabilit√© |
|---------|----------|---------|----------------|-----------|
| **A. Fusionner + z√©ro** | `merge-data` puis `train` | Lent | Aucun | Maximale |
| **B. Resume seul** | `train-resume` | Rapide | Mod√©r√© | Bonne |
| **C. Fusionner + resume** | `train-merged` | Moyen | Faible | Tr√®s bonne |

> **Conseil** : commencez par la m√©thode A pour vous familiariser. Si le temps d'entra√Ænement devient un probl√®me (datasets volumineux, gros mod√®les), passez √† la m√©thode C.

## Convertir en GGUF

```bash
make convert MODEL=unsloth/gemma-3-270m-it
```

Pour changer le type de quantisation :

```bash
make convert MODEL=unsloth/gemma-3-270m-it QUANTIZE=Q5_K_M
```

### Types de quantisation disponibles

| Type | Bits/poids | Qualit√© | Taille (0.5B) | Usage recommand√© |
|------|-----------|---------|---------------|------------------|
| `f16` | 16 | Maximale | ~963 MB | R√©f√©rence, pas pour la production |
| `Q8_0` | 8 | Excellente | ~530 MB | Quand la qualit√© prime |
| `Q5_K_M` | ~5.5 | Tr√®s bonne | ~450 MB | Bon compromis qualit√©/taille |
| `Q4_K_M` | ~4.5 | Bonne | ~394 MB | **Recommand√©** pour la plupart des cas |

### Pourquoi deux fichiers GGUF ?

La conversion produit **deux fichiers** dans `output/gguf/` :

```
output/gguf/
‚îú‚îÄ‚îÄ Qwen2.5-Coder-0.5B-Instruct-finetuned-f16.gguf      # 963 MB
‚îî‚îÄ‚îÄ Qwen2.5-Coder-0.5B-Instruct-finetuned-Q4_K_M.gguf   # 394 MB
```

**Le GGUF f16 (16-bit float)** est le mod√®le en pleine pr√©cision. Il est produit comme **√©tape interm√©diaire** de la conversion : les poids du mod√®le sont stock√©s tels quels, sans perte. Il sert de source pour la quantisation et peut servir de **r√©f√©rence qualit√©** si vous voulez comparer les r√©sultats avec et sans quantisation.

**Le GGUF quantis√© (Q4_K_M par d√©faut)** est le mod√®le compress√©. La quantisation r√©duit la pr√©cision des poids de 16 bits √† ~4.5 bits, ce qui divise la taille par ~2.5 avec une perte de qualit√© minime. C'est **celui que vous utiliserez en pratique** dans Ollama ou tout autre runtime.

En r√©sum√© : le f16 est la "version master", le Q4_K_M est la "version de distribution". Vous pouvez supprimer le f16 une fois satisfait de la qualit√© du quantis√©, ou le conserver pour requantiser avec un autre type (`Q5_K_M`, `Q8_0`...) sans relancer toute la conversion.

## Tester le mod√®le

> **Important** : les commandes `make test` et `make test-prompt` utilisent les **adapters LoRA** g√©n√©r√©s par le fine-tuning (dans `output/adapters/`). Vous devez sp√©cifier le **m√™me mod√®le** que celui utilis√© lors du `make train`. Si vous ne le sp√©cifiez pas, c'est le mod√®le par d√©faut (`Qwen/Qwen2.5-Coder-0.5B-Instruct`) qui est utilis√©. Si les adapters ont √©t√© entra√Æn√©s avec un autre mod√®le, vous obtiendrez une erreur `IndexError: list index out of range`.

### Test rapide

```bash
# Test avec 3 prompts par d√©faut
make test MODEL=unsloth/gemma-3-270m-it

# Test avec un prompt personnalis√©
make test-prompt MODEL=unsloth/gemma-3-270m-it PROMPT="Write a Go function that parses YAML"
```
Utilisez toujours le **m√™me `MODEL=`** que lors du `make train` :

### Erreur courante : `IndexError: list index out of range`

Cette erreur se produit quand le mod√®le sp√©cifi√© pour le test ne correspond pas √† celui qui a produit les adapters. Les adapters LoRA sont li√©s √† l'architecture exacte du mod√®le de base (nombre de couches, dimensions des tenseurs). Un adapter entra√Æn√© sur `Menlo/Lucy-128k` (28 couches, architecture Qwen3) ne peut pas √™tre charg√© sur `unsloth/Qwen2.5-0.5B-Instruct` (24 couches, architecture Qwen2).

**Pour corriger** :

```bash
# Option 1 : tester avec le bon mod√®le
# V√©rifiez dans output/adapters/adapter_config.json quel mod√®le a √©t√© utilis√©
cat output/adapters/adapter_config.json | grep '"model"'
# Puis utilisez ce mod√®le pour le test
make test-prompt MODEL=Menlo/Lucy-128k PROMPT="votre prompt"

# Option 2 : nettoyer et r√©-entra√Æner avec le mod√®le souhait√©
make clean-output
make train MODEL=unsloth/Qwen2.5-0.5B-Instruct
make test-prompt PROMPT="votre prompt"
```

### V√©rifier quel mod√®le a produit les adapters

Si vous ne vous souvenez plus quel mod√®le a √©t√© utilis√© pour le training :

```bash
cat output/adapters/adapter_config.json | grep '"model"'
# Affiche par exemple : "model": "Menlo/Lucy-128k"
```

Utilisez ensuite cette valeur dans `MODEL=` pour toutes les commandes suivantes (test, convert).

## Sortir de l'environnement Python

### Si vous utilisez uniquement `make`

Vous n'avez **rien √† faire**. Les scripts activent et utilisent le venv Python automatiquement √† chaque commande, de mani√®re isol√©e. Votre terminal n'est jamais "pollu√©" par le venv.

### Si vous avez activ√© le venv manuellement

Si vous avez activ√© le venv √† la main (pour explorer, d√©buguer, lancer `mlx_lm` directement, etc.) :

```bash
# Vous avez fait ceci √† un moment :
source .venv/bin/activate

# Votre prompt ressemble √† :
(.venv) $
```

Pour en sortir proprement :

```bash
deactivate
```

C'est tout. La commande `deactivate` est fournie automatiquement par Python quand un venv est actif. Elle restaure votre `PATH` d'origine et supprime le pr√©fixe `(.venv)` du prompt. Aucun processus √† tuer, aucun fichier √† nettoyer.

### V√©rifier que vous √™tes bien sorti

```bash
# Si le venv est actif, cette commande pointe vers .venv/bin/python
which python3

# Apr√®s deactivate, elle pointe vers le Python syst√®me :
# /usr/bin/python3 ou /opt/homebrew/bin/python3
```

### Nettoyage complet

Si vous avez termin√© vos exp√©riences de fine-tuning et voulez r√©cup√©rer l'espace disque :

```bash
# Supprimer uniquement les fichiers de sortie (adapters, mod√®les fusionn√©s, GGUF)
make clean-output

# Tout supprimer (venv Python, llama.cpp compil√©, et les sorties)
make clean
```

`make clean` supprime le r√©pertoire `.venv/` et `llama.cpp/`. Il faudra relancer `make setup` pour les recr√©er. Les fichiers du dataset (`data/`) et les scripts ne sont pas touch√©s.

## Commandes Make disponibles

| Commande | Description |
|----------|-------------|
| `make help` | Afficher l'aide et les variables configurables |
| `make setup` | Installer toutes les d√©pendances |
| `make train` | Lancer le fine-tuning LoRA |
| `make convert` | Convertir en GGUF (fuse + convert + quantize) |
| `make test` | Tester avec des prompts Go par d√©faut |
| `make test-prompt PROMPT="..."` | Tester avec un prompt personnalis√© |
| `make all` | Pipeline complet : train + convert + test |
| `make merge-data INPUTS="..." ` | Fusionner plusieurs datasets |
| `make train-resume` | Reprendre un entra√Ænement avec des adapters existants |
| `make train-merged INPUTS="..."` | Fusionner des datasets puis reprendre l'entra√Ænement |
| `make clean-output` | Supprimer les fichiers de sortie |
| `make clean` | Tout supprimer (output + venv + llama.cpp) |

## Structure du projet

```
donkeytune/
‚îú‚îÄ‚îÄ Makefile              # Point d'entr√©e principal
‚îú‚îÄ‚îÄ setup.sh              # Installation des d√©pendances
‚îú‚îÄ‚îÄ fine-tune.sh          # Fine-tuning LoRA via mlx-lm
‚îú‚îÄ‚îÄ merge-datasets.sh     # Fusion de plusieurs datasets
‚îú‚îÄ‚îÄ convert-to-gguf.sh    # Conversion fuse ‚Üí GGUF ‚Üí quantize
‚îú‚îÄ‚îÄ test-model.sh         # Test du mod√®le fine-tun√©
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ train.jsonl       # Dataset d'entra√Ænement
‚îÇ   ‚îî‚îÄ‚îÄ valid.jsonl       # Dataset de validation
‚îî‚îÄ‚îÄ output/
    ‚îú‚îÄ‚îÄ adapters/         # Adapters LoRA produits par le fine-tuning
    ‚îú‚îÄ‚îÄ fused/            # Mod√®le fusionn√© (adapters + base)
    ‚îî‚îÄ‚îÄ gguf/             # Fichiers GGUF finaux
```

## Mod√®les test√©s et compatibles

Le pipeline fonctionne avec des mod√®les HuggingFace compatibles mlx-lm. Le mod√®le doit √™tre au format **safetensors** (le format standard HuggingFace), **pas au format GGUF**.

> Les temps indiqu√©s ci-dessous sont estim√©s pour **100 exemples, 600 it√©rations** sur un MacBook Air M4 24 GB.

### Gemma 3 270M-IT (0.3B, le plus petit, id√©al pour les tests rapides)

| | |
|---|---|
| **HuggingFace** | `unsloth/gemma-3-270m-it` |
| **Taille** | 0.3B param√®tres (270M) |
| **Architecture** | Gemma 3 |
| **Couches** | 18 |
| **Contexte** | 32K tokens |
| **M√©moire utilis√©e** | ~2-3 GB |
| **Temps de fine-tuning** | ~5 minutes |
| **Taille GGUF Q4_K_M estim√©e** | ~250 MB |

```bash
make train MODEL=unsloth/gemma-3-270m-it
make convert MODEL=unsloth/gemma-3-270m-it
make test-prompt MODEL=unsloth/gemma-3-270m-it PROMPT="Who is Jean Luc Picard?"
```

Le plus petit mod√®le de cette liste, d√©velopp√© par Google. Sa taille minuscule en fait un candidat id√©al pour tester rapidement le pipeline ou pour les machines avec tr√®s peu de RAM (8 GB). La qualit√© sera inf√©rieure aux mod√®les 0.5B+, mais l'entra√Ænement est extr√™mement rapide. Aucun ajustement de param√®tres n√©cessaire, les valeurs par d√©faut fonctionnent bien.

> **Gemma 3 270M vs Qwen2.5 0.5B** : Gemma 3 270M est ~40% plus petit et s'entra√Æne plus vite, mais produit des r√©sultats de moindre qualit√©. Utilisez Gemma pour it√©rer et tester rapidement, Qwen 0.5B pour une meilleure qualit√© de sortie tout en restant l√©ger.

---

### Qwen2.5-Coder-0.5B-Instruct

| | |
|---|---|
| **HuggingFace** | `Qwen/Qwen2.5-Coder-0.5B-Instruct` |
| **Taille** | 0.5B param√®tres |
| **Architecture** | Qwen2 |
| **M√©moire utilis√©e** | ~3.7 GB |
| **Temps de fine-tuning** | ~8 minutes |
| **Temps de conversion GGUF** | ~30 secondes |
| **Taille GGUF Q4_K_M** | ~394 MB |

```bash
make train MODEL=Qwen/Qwen2.5-Coder-0.5B-Instruct
make convert MODEL=Qwen/Qwen2.5-Coder-0.5B-Instruct
make test-prompt MODEL=Qwen/Qwen2.5-Coder-0.5B-Instruct PROMPT="say hello world in golang"
```

Aucun ajustement n√©cessaire, les param√®tres par d√©faut sont optimaux.

---

### Qwen2.5-0.5B-Instruct (0.5B, g√©n√©raliste multilingue, recommand√© pour d√©buter)

| | |
|---|---|
| **HuggingFace** | `unsloth/Qwen2.5-0.5B-Instruct` |
| **Taille** | 0.5B param√®tres |
| **Architecture** | Qwen2 (RoPE, SwiGLU, GQA 14Q/2KV) |
| **Couches** | 24 |
| **Contexte** | 32K tokens |
| **M√©moire utilis√©e** | ~3.7 GB |
| **Temps de fine-tuning** | ~8 minutes |
| **Temps de conversion GGUF** | ~30 secondes |
| **Taille GGUF Q4_K_M estim√©e** | ~400 MB |

```bash
make train MODEL=unsloth/Qwen2.5-0.5B-Instruct
make convert MODEL=unsloth/Qwen2.5-0.5B-Instruct
make test-prompt MODEL=unsloth/Qwen2.5-0.5B-Instruct PROMPT="Who is Seven of Nine?"
```

Version g√©n√©raliste du Qwen2.5-0.5B. M√™me architecture et m√™mes param√®tres que `Qwen/Qwen2.5-Coder-0.5B-Instruct`, mais pr√©-entra√Æn√© sur des donn√©es g√©n√©ralistes multilingues (29+ langues) plut√¥t que sur du code. Aucun ajustement n√©cessaire, les param√®tres par d√©faut sont optimaux.

> **Qwen2.5-Coder-0.5B vs Qwen2.5-0.5B** : architecture identique, taille identique, m√™me m√©moire, m√™me vitesse. Choisissez le Coder si votre dataset porte sur la g√©n√©ration de code, le g√©n√©raliste sinon (Q&A, r√©daction, dialogue).

---

### Qwen2.5-1.5B-Instruct (1.5B, g√©n√©raliste multilingue)

| | |
|---|---|
| **HuggingFace** | `unsloth/Qwen2.5-1.5B-Instruct` |
| **Taille** | 1.54B param√®tres (1.31B non-embedding) |
| **Architecture** | Qwen2 (RoPE, SwiGLU, GQA 12Q/2KV) |
| **Couches** | 28 |
| **Contexte** | 32K tokens |
| **M√©moire estim√©e** | ~5-7 GB |
| **Temps de fine-tuning** | ~15-20 minutes |
| **Temps de conversion GGUF** | ~1 minute |
| **Taille GGUF Q4_K_M estim√©e** | ~1 GB |

```bash
make train MODEL=unsloth/Qwen2.5-1.5B-Instruct
make convert MODEL=unsloth/Qwen2.5-1.5B-Instruct
make test-prompt MODEL=unsloth/Qwen2.5-1.5B-Instruct PROMPT="Who is James Tiberius Kirk?"
```

Mod√®le g√©n√©raliste multilingue (29+ langues). Bon choix pour du Q&A, de la r√©daction, ou des t√¢ches non sp√©cifiquement li√©es au code. Les param√®tres par d√©faut conviennent.

---

### Qwen2.5-Coder-1.5B-Instruct (1.5B, sp√©cialis√© code)

| | |
|---|---|
| **HuggingFace** | `unsloth/Qwen2.5-Coder-1.5B-Instruct` |
| **Taille** | 1.54B param√®tres (1.31B non-embedding) |
| **Architecture** | Qwen2 (RoPE, SwiGLU, GQA 12Q/2KV) |
| **Couches** | 28 |
| **Contexte** | 128K tokens (via YaRN) |
| **M√©moire estim√©e** | ~5-7 GB |
| **Temps de fine-tuning** | ~15-20 minutes |
| **Temps de conversion GGUF** | ~1 minute |
| **Taille GGUF Q4_K_M estim√©e** | ~1 GB |

```bash
make train MODEL=unsloth/Qwen2.5-Coder-1.5B-Instruct
make convert MODEL=unsloth/Qwen2.5-Coder-1.5B-Instruct
make test-prompt MODEL=unsloth/Qwen2.5-Coder-1.5B-Instruct PROMPT="Write a Go HTTP server with middleware"
```

Version code du Qwen2.5-1.5B. M√™me architecture, mais pr√©-entra√Æn√© sur du code. C'est un excellent choix pour le fine-tuning sur la g√©n√©ration de code : il combine une taille raisonnable (1.5B) avec une sp√©cialisation code et un contexte long (128K). Les param√®tres par d√©faut conviennent.

> **Qwen2.5-1.5B vs Qwen2.5-Coder-1.5B** : m√™me architecture, m√™me taille, m√™me param√®tres de fine-tuning. La seule diff√©rence est le pr√©-entra√Ænement : le Coder est meilleur pour g√©n√©rer du code, le g√©n√©raliste est meilleur pour du Q&A ou de la r√©daction. Choisissez en fonction de votre dataset.

---

### SmolLM3-3B-128K (3B, le maximum confortable)

| | |
|---|---|
| **HuggingFace** | `unsloth/SmolLM3-3B-128K` |
| **Taille** | 3B param√®tres |
| **Architecture** | SmolLM3 |
| **M√©moire estim√©e** | ~12-14 GB |
| **Temps de fine-tuning** | ~40-50 minutes |
| **Temps de conversion GGUF** | ~2 minutes |
| **Taille GGUF Q4_K_M estim√©e** | ~1.8 GB |

```bash
make train MODEL=unsloth/SmolLM3-3B-128K BATCH_SIZE=2 NUM_LAYERS=4
make convert MODEL=unsloth/SmolLM3-3B-128K
```

R√©duire `BATCH_SIZE` et `NUM_LAYERS` pour rester dans les 24 GB.

---

### R√©capitulatif

| Mod√®le | Taille | M√©moire | Temps estim√© | Confort M4 24GB |
|--------|--------|---------|-------------|-----------------|
| `unsloth/gemma-3-270m-it` | 0.3B | ~2-3 GB | ~5 min | Tr√®s confortable |
| `Qwen/Qwen2.5-Coder-0.5B-Instruct` | 0.5B | ~3.7 GB | ~8 min | Tr√®s confortable |
| `unsloth/Qwen2.5-0.5B-Instruct` | 0.5B | ~3.7 GB | ~8 min | Tr√®s confortable |
| `unsloth/Qwen2.5-1.5B-Instruct` | 1.5B | ~5-7 GB | ~15-20 min | Tr√®s confortable |
| `unsloth/Qwen2.5-Coder-1.5B-Instruct` | 1.5B | ~5-7 GB | ~15-20 min | Tr√®s confortable |
| `unsloth/SmolLM3-3B-128K` | 3B | ~12-14 GB | ~40-50 min | OK (r√©duire params) |

> Mod√®le Base (non-Instruct) : n√©cessite un dataset plus grand et plus d'it√©rations pour apprendre le format instruction/r√©ponse.

Le mod√®le est t√©l√©charg√© automatiquement depuis HuggingFace lors du premier lancement. Le premier `make train` avec un nouveau mod√®le prendra quelques minutes suppl√©mentaires pour le t√©l√©chargement (comptez ~1 GB pour un mod√®le 0.5B, ~3 GB pour un mod√®le 1.5B, ~8 GB pour un mod√®le 4B, ~16 GB pour un mod√®le 8B).
